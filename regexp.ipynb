{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정규표현식(regular expression)은 집합 표현과 같은 형태로 이해."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Basic Regular Expression Meta-Characters, Including Wildcards, Ranges and Closures\n",
    ".\t        Wildcard, matches any character\n",
    "^abc\t    Matches some pattern abc at the start of a string\n",
    "abc$\t    Matches some pattern abc at the end of a string\n",
    "[abc]\t    Matches one of a set of characters\n",
    "[^abc]      Matches anything but a set of characters\n",
    "[A-Z0-9]\tMatches one of a range of characters\n",
    "ed|ing|s\tMatches one of the specified strings (disjunction)\n",
    "*\t        Zero or more of previous item, e.g. a*, [a-z]* (also known as Kleene Closure)\n",
    "+\t        One or more of previous item, e.g. a+, [a-z]+\n",
    "?\t        Zero or one of the previous item (i.e. optional), e.g. a?, [a-z]?\n",
    "{n}\t        Exactly n repeats where n is a non-negative integer\n",
    "{n,}\t    At least n repeats\n",
    "{,n}\t    No more than n repeats\n",
    "{m,n}\t    At least m and no more than n repeats\n",
    "a(b|c)+\t    Parentheses that indicate the scope of the operators\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\fftth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "dic = nltk.corpus.words.words('en')\n",
    "print(type(dic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'aa', 'aal', 'aalii', 'aam', 'aardvark', 'aardwolf', 'aba', 'abac', 'abaca']\n"
     ]
    }
   ],
   "source": [
    "wordlist = [w for w in dic if w.islower()]\n",
    "print(wordlist[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "re.search함수는 괄호 안에 '찾고자 하는 표현식'과 '대상'이라는 2개의 요소를 입력. abcde라는 대상에서 abc라는 string을 찾을 때 사용 가능. 이때 re.search는 string에만 적용 가능."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 3), match='abc'>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search('abc', 'abcde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for루프에서는 in 다음에 있는 것부터 파악. w에 있는 항목 수 만큼 for루프가 도는데 if조건에 부합하면 wordlist에 쌓기."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'aa', 'aal', 'aalii', 'aam', 'aardvark', 'aardwolf', 'aba', 'abac', 'abaca']\n",
      "['abaissed', 'abandoned', 'abased', 'abashed', 'abatised', 'abed', 'aborted', 'abridged', 'abscessed', 'absconded']\n",
      "['abjectly', 'adjuster', 'dejected', 'dejectly', 'injector', 'majestic', 'objectee', 'objector', 'rejecter', 'rejector']\n",
      "['gold', 'golf', 'hold', 'hole']\n",
      "['a', 'aa', 'ah', 'aha', 'h', 'ha', 'hah']\n"
     ]
    }
   ],
   "source": [
    "wordlist = [w for w in nltk.corpus.words.words('en') if w.islower()]\n",
    "print(wordlist[:10])\n",
    "result = [w for w in wordlist if re.search('ed$', w)][:10]\n",
    "print(result[:10])\n",
    "result = [w for w in wordlist if re.search('^..j..t..$', w)][:10]\n",
    "print(result[:10])\n",
    "result = [w for w in wordlist if re.search('^[ghi][mno][jlk][def]$', w)][:10]\n",
    "print(result[:10])\n",
    "result = [w for w in wordlist if re.search('^[ah]+$', w)][:10]\n",
    "print(result[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\fftth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\treebank.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('treebank')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[]안에서 규정하는 range 내에서 하나만 찾아 주는 것. 그리고 그것이 반복 되는 것.\n",
    "^이 대괄호 안에서 사용 될 때는 특정 문자를 제외하고 찾으라는 것.\n",
    "소괄호는 묶음을 의미할 뿐."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0.0085', '0.05', '0.1', '0.16', '0.2', '0.25', '0.28', '0.3', '0.4', '0.5']\n",
      "['C$', 'US$']\n",
      "['1614', '1637', '1787', '1901', '1903', '1917', '1925', '1929', '1933', '1934']\n",
      "['10-day', '10-lap', '10-year', '100-share', '12-point', '12-year', '14-hour', '15-day', '150-point', '190-point']\n",
      "['black-and-white', 'bread-and-butter', 'father-in-law', 'machine-gun-toting', 'savings-and-loan']\n",
      "['62%-owned', 'Absorbed', 'According', 'Adopting', 'Advanced', 'Advancing', 'Alfred', 'Allied', 'Annualized', 'Anything']\n"
     ]
    }
   ],
   "source": [
    "wsj = sorted(set(nltk.corpus.treebank.words()))\n",
    "wordlist = [w for w in wsj if re.search('^[0-9]+\\.[0-9]+$', w)]\n",
    "print(wordlist[:10])\n",
    "wordlist = [w for w in wsj if re.search('^[A-Z]+\\$$', w)]\n",
    "print(wordlist[:10])\n",
    "wordlist = [w for w in wsj if re.search('^[0-9]{4}$', w)]\n",
    "print(wordlist[:10])\n",
    "wordlist = [w for w in wsj if re.search('^[0-9]+-[a-z]{3,5}$', w)]\n",
    "print(wordlist[:10])\n",
    "wordlist = [w for w in wsj if re.search('^[a-z]{5,}-[a-z]{2,3}-[a-z]{,6}$', w)]\n",
    "print(wordlist[:10])\n",
    "wordlist = [w for w in wsj if re.search('(ed|ing)$', w)]\n",
    "print(wordlist[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ab', 'ab', 'ab', 'ab']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('ab', 'ababdddabkabddd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('io', 549),\n",
       " ('ea', 476),\n",
       " ('ie', 331),\n",
       " ('ou', 329),\n",
       " ('ai', 261),\n",
       " ('ia', 253),\n",
       " ('ee', 217),\n",
       " ('oo', 174),\n",
       " ('ua', 109),\n",
       " ('au', 106)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist = nltk.FreqDist(vs for word in wsj for vs in re.findall(r'[aeiou]{2,}', word))\n",
    "fdist.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abaissed', 'abandoned', 'abased', 'abashed', 'abatised', 'abed', 'aborted', 'abridged', 'abscessed', 'absconded']\n"
     ]
    }
   ],
   "source": [
    "result = [w for w in wordlist if re.search('ed$', w)]\n",
    "print(result[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['u', 'e', 'a', 'i', 'a', 'i', 'i', 'i', 'e', 'i', 'a', 'i', 'o', 'i', 'o', 'u']\n",
      "['rc', 'fr', 'st', 'xp', 'ci']\n",
      "['supercalifragilisticexpialidocious']\n",
      "['sup', 'rcal', 'frag', 'lis', 'tic', 'xpial', 'doc']\n"
     ]
    }
   ],
   "source": [
    "word = 'supercalifragilisticexpialidocious'\n",
    "result = re.findall(r'[aeiou]', word)\n",
    "print(result)\n",
    "result = re.findall('[aeiou](..)[aeiou]', word)\n",
    "print(result)\n",
    "result = re.findall('[^aeiou].+[^aeiou]', word)\n",
    "print(result)\n",
    "result = re.findall('[^aeiou].+?[^aeiou]', word)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('io', 549),\n",
       " ('ea', 476),\n",
       " ('ie', 331),\n",
       " ('ou', 329),\n",
       " ('ai', 261),\n",
       " ('ia', 253),\n",
       " ('ee', 217),\n",
       " ('oo', 174),\n",
       " ('ua', 109),\n",
       " ('au', 106)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wsj = sorted(set(nltk.corpus.treebank.words()))\n",
    "fdist = nltk.FreqDist(vs for word in wsj for vs in re.findall(r'[aeiou]{2,}', word))\n",
    "fdist.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    a   e   i   o   u \n",
      "k 418 148  94 420 173 \n",
      "p  83  31 105  34  51 \n",
      "r 187  63  84  89  79 \n",
      "s   0   0 100   2   1 \n",
      "t  47   8   0 148  37 \n",
      "v  93  27 105  48  49 \n"
     ]
    }
   ],
   "source": [
    "regexp = r'^[AEIOUaeiou]+|[AEIOUaeiou]+$|[^AEIOUaeiou]'\n",
    "def compress(word):\n",
    "    pieces = re.findall(regexp, word)\n",
    "    return ''.join(pieces)\n",
    "\n",
    "english_udhr = nltk.corpus.udhr.words('English-Latin1')\n",
    "print(nltk.tokenwrap(compress(w) for w in english_udhr[:75]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    a   e   i   o   u \n",
      "k 418 148  94 420 173 \n",
      "p  83  31 105  34  51 \n",
      "r 187  63  84  89  79 \n",
      "s   0   0 100   2   1 \n",
      "t  47   8   0 148  37 \n",
      "v  93  27 105  48  49 \n"
     ]
    }
   ],
   "source": [
    "rotokas_words = nltk.corpus.toolbox.words('rotokas.dic')\n",
    "cvs = [cv for w in rotokas_words for cv in re.findall(r'[ptksvr][aeiou]', w)]\n",
    "cfd = nltk.ConditionalFreqDist(cvs) # CFD works pairwise only\n",
    "cfd.tabulate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_word_pairs = [(cv, w) for w in rotokas_words for cv in re.findall(r'[ptksvr][aeiou]', w)]\n",
    "cv_index = nltk.Index(cv_word_pairs)\n",
    "cv_index['po']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
